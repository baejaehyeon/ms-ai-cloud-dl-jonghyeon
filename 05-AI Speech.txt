AI Speech : 인지 서비스중 하나로, 애플리케이션에 음성 기반 기능을 통합할 수 있게 해주는 클라우드 기반 서비스.
음성-텍스트 변환 및 텍스트-음성 변환 기술을 통해 사용자의 음성을 인식하고 처리하며, 자연스러운 음성 응답을 생성하는 기능을 제공.
* 아래는 AI Foundry에 통합되기 이전의, Speech Studio에서 음성 파일을 업로드하여 텍스트로 변환하는 실습 순서

* 리소스 생성 및 실습 순서
1. Azure 리소스 "speech" 검색 후, 목록 중 "음성" 선택하여 생성
2. Region은 East US 선택 후 리소스 생성
3. 가격 책정은 Standard 선택 후 만들기
4. 리소스로 이동 후, 개요 탭의 Key와 EndPoint 정보 확인
5. 리소스로 이동 후, 개요 탭 화면 중앙 "Speech Studio 이동" 클릭하여 이동
6. Speech Studio 우측 상단 "이전 디자인으로 전환"하여, Foundry에 통합되기 이전의 디자인 화면으로 전환
7. 하단 적절한 모델 선택 후, 음성 파일 업로드하여 테스트

* 주요 기능
1) 음성-텍스트 변환 (Speech-to-Text)
음성 인식이라고도 불리며, 사람이 말하는 음성 오디오를 텍스트로 변환하는 기능.
실시간 STT : 라이브 오디오 스트림에서 즉시 음성을 인식하고 텍스트를 반환. 회의나 라이브 방송 자막, 음성 비서와의 대화 등에 사용.
비실시간 STT : 저장된 오디오 파일을 업로드하여 텍스트로 변환.
화자 다이아리제이션 : 여러 사람이 대화하는 오디오에서 "누가 언제 말했는지"를 식별하여, 발화 구간별로 화자를 구분해 텍스트에 표시.
사용자 지정 음향 모델: 특정 환경(예: 시끄러운 공장)이나 용어(예: 의학, 법률 전문 용어)에 맞춰 정확도를 향상시키기 위해 모델을 학습시킬 수 있음.

2) 텍스트-음성 변환 (Text-to-Speech, TTS)
텍스트를 입력하면 자연스럽게 들리는 사람의 목소리로 합성하여 출력하는 기능.
신경망 TTS (Neural TTS): 사람의 말과 거의 구별할 수 없을 정도로 자연스러운 합성 음성을 생성하는 최첨단 기술. 억양, 감정, 발음의 품질이 매우 높음.
사용자 지정 음성 (Custom Voice): 특정 브랜드나 화자의 고유한 목소리를 복제하여 TTS 모델을 만들 수 있음. (예: 기업 CEO의 목소리를 복제하여 안내 음성 서비스에 사용)
다국어 및 다양한 목소리: 수십 개 언어를 지원하며, 각 언어별로 다양한 성별과 스타일의 목소리를 선택할 수 있음.

3) 음성 번역 (Speech Translation)
실시간으로 한 언어의 음성을 인식하고 이를 다른 언어의 텍스트 또는 음성으로 변환하는 기능.
실시간 양방향 번역: 회의나 대화에서 실시간으로 통역이 필요한 경우 유용하며, 음성 입력 후 텍스트로 변환하고 다시 목표 언어의 음성으로 출력할 수 있음.
다국어 지원: 여러 언어 간의 번역을 지원하여 글로벌 커뮤니케이션 환경을 지원.

================================================================================
"실시간 음성 텍스트 변환"(Batch) - REST API - PostMan 테스트
================================================================================
* 요청은 POST(요청) => GET(Content URL 확인) => GET(Content 확인)으로 3단계에 나누어 이루어진다.

* 샘플 코드
curl -v -X POST -H "Ocp-Apim-Subscription-Key: {API-KEY}" -H "Content-Type: application/json" -d '{
  "displayName": "20251203_172143",
  "description": "Speech Studio Batch speech to text",
  "locale": "ko-kr",
  "contentUrls": [
    "https://sstudioprodsause.blob.core.windows.net/tempfiles/f373cf07-e4c2-4c8a-95bc-b09a654cda84?skoid=3fdd8927-6ff6-4248-a3c8-246cacfd5da8&sktid=33e01921-4d64-4f8c-a055-5bdaffd5e33d&skt=2025-12-03T08%3A22%3A28Z&ske=2025-12-04T08%3A22%3A28Z&sks=b&skv=2023-11-03&sv=2023-11-03&st=2025-12-03T08%3A22%3A28Z&se=2025-12-05T08%3A22%3A28Z&sr=b&sp=r&sig=DW5VT18YYXZ4naK1wfydR5d0sInZbyB5jTuFhuRaHog%3D&speechstudiofilename=guide.mp3"
  ],
  "model": {
    "self": "https://eastus.api.cognitive.microsoft.com/speechtotext/v3.2/models/base/a259820b-7626-4e6a-9a24-09d60874c6c9"
  },
  "properties": {
    "wordLevelTimestampsEnabled": false,
    "displayFormWordLevelTimestampsEnabled": true,
    "diarizationEnabled": false,
    "punctuationMode": "DictatedAndAutomatic",
    "profanityFilterMode": "Masked"
  },
  "customProperties": {}
}' "https://eastus.api.cognitive.microsoft.com/speechtotext/v3.2/transcriptions"

* 실습 순서
(1) POST(요청)
- Endpoint : https://eastus.api.cognitive.microsoft.com/speechtotext/v3.2/transcriptions
- Method : POST
- Header 정보 : Ocp-Apim-Subscription-Key은 본인 API-KEY, Content-Type은 application/json
- Body 정보 : 위 샘플코드 중, -d 이후 중괄호 모두 복사 (샘플코드 마지막 endPoint 정보 제외)
- 응답 JSON 중, {"links" : { "files" : url정보 } }에서 url정보가 다음 GET(Content URL 확인) 요청의 endpoint가 됨.

(2) GET(Content URL 확인) => 실제 코드에서는 응답 상태가 Succeeded일 때까지 반복 요청 처리 필요.
- Endpoint : 위 POST 요청 응답의 url 정보 ([Ex] https://eastus.api.cognitive.microsoft.com/speechtotext/v3.2/transcriptions/c76e8c32-e794-4779-abf6-daefbd1b7fa9/files)
- Method : GET
- Header 정보 : Ocp-Apim-Subscription-Key은 본인 API-KEY
- Body 정보 : 없음
- 응답 JSON 중, values의 0번째 요소 내에 "contentUrl"의 value 값이 다음 GET(Content 확인) 요청의 endpoint가 됨.
  만약, 요청 음성 파일이 여러개인경우, values의 마지막 요소를 제외한 요소들이 각 음성파일의 결과 정보가 됨.

(3) GET(Content 확인)
- Endpoint : 위 GET 요청 응답의 contentUrl 정보 ([Ex] https://spsvcprodeus.blob.core.windows.net/bestor-c6e3ae79-1b48-41bf-92ff-940bea3e5c2d/TranscriptionData/c76e8c32-e794-4779-abf6-daefbd1b7fa9_0_0.json?skoid=50c6251a-ac54-47a3-9265-a1e4f84be9b9&sktid=33e01921-4d64-4f8c-a055-5bdaffd5e33d&skt=2025-12-03T08%3A35%3A47Z&ske=2025-12-08T08%3A40%3A47Z&sks=b&skv=2021-08-06&sv=2025-01-05&st=2025-12-03T08%3A35%3A47Z&se=2025-12-03T20%3A40%3A47Z&sr=b&sp=rl&sig=qgMABcze7tQ4Ardkh7rwRj9ik9q5F6sV%2BNzXHcvhGMA%3D)
- Method : GET
- Header 정보 : Ocp-Apim-Subscription-Key은 본인 API-KEY
- Body 정보 : 없음
- 응답 JSON이, 음성을 텍스트로 변환한 결과이며, 아래 예시와 같이 4개의 결과가 출력되는데, 
  각 정규화 단계를 거쳐 최종 사용자에게 보여질 텍스트인 display로 보여주면 됨..

"lexical": "안녕하세요 시각 장애인을 위한 the mins i g 입니다 생성을 원하시는 이미지에 대해 설명해 주세요",
"itn": "안녕하세요 시각 장애인을 위한 the mins IG 입니다 생성을 원하시는 이미지에 대해 설명해 주세요",
"maskedITN": "안녕하세요 시각 장애인을 위한 the mins ig 입니다 생성을 원하시는 이미지에 대해 설명해 주세요",
"display": "안녕하세요. 시각 장애인을 위한 the mins IG 입니다. 생성을 원하시는 이미지에 대해 설명해 주세요."

1. lexical : 모든 구두점과 대소문자 표기 없이 말 그대로의 단어만 나열합니다. 주로 기계 학습 모델의 추가 분석이나 검색 엔진 인덱싱을 위한 원본 입력으로 사용됩니다.	the mins i g → the mins i g
2. itn : 역 텍스트 정규화 (Inverse Text Normalization) 적용 숫자, 약어, 통화 단위 등 사람이 읽기 쉽게 발화한 내용을 일반적인 문서 형식으로 되돌립니다. 구두점은 여전히 포함되지 않습니다.	아이 지 → IG / 백 이십 → 120
3. maskedITN : ITN 적용 + 비속어/민감어 마스킹	itn 결과에 기반하여, 설정된 비속어, 욕설, 민감한 단어 등을 지정된 마스크 문자(예: ***)로 대체합니다.	민감 단어 → ***
4. display	최종 사용자에게 표시할 형식	문장의 끝에 구두점을 추가하고, 첫 단어를 대문자로 바꾸며, 약어 등을 최종적으로 정제하여 가독성이 가장 높은 형태로

================================================================================
"STT(Speech To Text)" - REST API - PostMan 테스트
================================================================================
* 요청은 POST(Content 확인 요청)으로 이루어진다. 위 Batch 작업과 다르게 1단계로 이루어짐.
* 참고 URL : https://learn.microsoft.com/ko-kr/azure/ai-services/speech-service/get-started-speech-to-text?tabs=new-foundry%2Cwindows&pivots=programming-language-rest

* 샘플 코드 (%SPEECH_REGION%에는 speech 리소스 생성 지역명을 입력해야 함. => "eastus", 음성 파일을 body에 업로드)
curl --location --request POST "https://%SPEECH_REGION%.stt.speech.microsoft.com/speech/recognition/conversation/cognitiveservices/v1?language=en-US&format=detailed" ^
--header "Ocp-Apim-Subscription-Key: %SPEECH_KEY%" ^
--header "Content-Type: audio/wav" ^
--data-binary "@YourAudioFile.wav"

* 실습 순서
(1) POST(Content 확인 요청)
- Endpoint : https://eastus.stt.speech.microsoft.com/speech/recognition/conversation/cognitiveservices/v1?language=en-US&format=detailed
- Method : POST
- Header 정보 : Ocp-Apim-Subscription-Key은 본인 API-KEY, Content-Type은 audio/wav
- Body 정보 : Body 탭의 binary 선택 후 , 음성 파일 업로드

* 결과 샘플
{
    "RecognitionStatus": "Success",
    "Offset": 3500000,
    "Duration": 86000000,
    "DisplayText": "페르소나는 이후에 고객 대응을 할 때 회사의 대표 이미지가 될 것이기 때문에 잘 고려해서 결정하셔야 합니다.",
    "NBest": [
        {
            "Confidence": 0.93333817,
            "Lexical": "페르소나는 이후에 고객 대응을 할 때 회사의 대표 이미지가 될 것이기 때문에 잘 고려해서 결정하셔야 합니다",
            "ITN": "페르소나는 이후에 고객 대응을 할 때 회사의 대표 이미지가 될 것이기 때문에 잘 고려해서 결정하셔야 합니다",
            "MaskedITN": "페르소나는 이후에 고객 대응을 할 때 회사의 대표 이미지가 될 것이기 때문에 잘 고려해서 결정하셔야 합니다",
            "Display": "페르소나는 이후에 고객 대응을 할 때 회사의 대표 이미지가 될 것이기 때문에 잘 고려해서 결정하셔야 합니다."
        }
    ]
}

================================================================================
"TTS(Text To Speech)" - REST API - PostMan 테스트
================================================================================
* 요청은 POST(Content 확인 요청)으로 이루어진다.
* 참고 URL : https://learn.microsoft.com/ko-kr/azure/ai-services/speech-service/get-started-text-to-speech?tabs=windows&pivots=programming-language-rest

* 샘플 코드
curl --location --request POST "https://%SPEECH_REGION%.tts.speech.microsoft.com/cognitiveservices/v1" ^
--header "Ocp-Apim-Subscription-Key: %SPEECH_KEY%" ^
--header "Content-Type: application/ssml+xml" ^
--header "X-Microsoft-OutputFormat: audio-16khz-128kbitrate-mono-mp3" ^
--header "User-Agent: curl" ^
--data-raw "<speak version='1.0' xml:lang='en-US'><voice xml:lang='en-US' xml:gender='Female' name='en-US-Ava:DragonHDLatestNeural'>my voice is my passport verify me</voice></speak>"

* 실습 순서
(1) POST(Speech 음성 파일 요청)
- Endpoint : https://eastus.tts.speech.microsoft.com/cognitiveservices/v1
- Method : POST
- Header 정보 : Ocp-Apim-Subscription-Key은 본인 API-KEY, Content-Type은 application/ssml+xml, X-Microsoft-OutputFormat은 audio-16khz-128kbitrate-mono-mp3 입력
               + SSML(Speech Synthesis Markup Language)이란, XML 기반 태그 언어인데 일반 텍스트만으로는 표현하기 힘든 억양, 속도, 음량, 발음, 감정과 같은 음성 합성의 세부 사항을 프로그래밍 방식으로 지정할 수 있음.
                 참고 URL : https://learn.microsoft.com/ko-kr/azure/ai-services/speech-service/speech-synthesis-markup-structure
               + X-Microsoft-OutputFormat은 서버가 합성하여 반환할 음성 오디오 파일의 형식(Format)과 품질(Quality)을 의미한다. 윈도우에서는 mp3가 잘 안될 수 있으니, 그때는 pcm으로 설정하여 wav 형태의 파일로 받자.
                 riff-8khz-16bit-mono-pcm
                 riff-16khz-16bit-mono-pcm
                 riff-24khz-16bit-mono-pcm
                 riff-48khz-16bit-mono-pcm
                 audio-16khz-32kbitrate-mono-mp3
                 audio-16khz-64kbitrate-mono-mp3
                 audio-16khz-128kbitrate-mono-mp3
                 audio-24khz-48kbitrate-mono-mp3
                 audio-24khz-96kbitrate-mono-mp3
                 audio-24khz-160kbitrate-mono-mp3
- Body 정보 : Body 탭의 raw 선택 및 XML 선택 후, <speak version='1.0' xml:lang='en-US'><voice xml:lang='en-US' xml:gender='Female' name='en-US-Ava:DragonHDLatestNeural'>my voice is my passport verify me</voice></speak> 입력
             https://json2video.com/ai-voices/azure/voices/ 사이트에서, 목소리별 코드 확인 가능
